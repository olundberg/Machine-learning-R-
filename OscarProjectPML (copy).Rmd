---
title: "Course Project in Practical Machine Learning at Johns Hopkins University"
author: "Oscar Lundberg"
date: '2015-02-08'
output: html_document
---

As a part of completing the course in Practical Machine Learning at Johns Hopkins University the student should complete a smaller project in Machine Learning. The project should then be presented in a compiled HTML.

I am currently a third year student in M.Sc. Engineering Physics and Electrical Engineering at LTU in Sweden. Please, feel free to contact me at lundberg.oscar@gmail.com. LinkedIn profile https://www.linkedin.com/in/oscarlundberg.

## 1. Project
*This is the exercise given. The goal of the project is to predict the manner in which the participants performed the exercise.*

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

### 1.1 Data
The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

## 2. Machine Learning
The project is done using R and RStudio running R version 3.1.2 on 64-bit Ubuntu (14.04). 

The procedure of predicting the outcome starts by importing library's and data. The data is then "cleaned" in order to build the models with relevant data to avoid overfitting. After that different training methods are used to build models.

In order to evaluate how well the different models are performing the training data is sliced into one training set and one test set. The training set is used to build the models and the models are then used on the testing set to evaluate the models.

Finally the models are run on the real test set where I will use the best model to present my predictions.

### 2.1 Importing library's, data and set seed
I begin by importing the library's needed. Caret is the package used for machine learning here.

Since the program depends on random variables we use the set.seed() in order to get reproducible results. This ensures us that the random samplers will sample the same variables.
```{r}
## 2.1 Importing library's, data and set seed
library(caret)
set.seed(12345)
train <- read.csv("pml-training.csv",na.strings=c("NA",""))
testing <- read.csv("pml-testing.csv",na.strings=c("NA",""))
```

### 2.2 Cleaning the data
In order to build a good predictor we need to clean the data. The first step is to clean out columns with many Na's. I have chosen to clean out columns where the Na's are more than 15 %. This is somewhat arbitrary. Due to the relatively big data set (19622 obs.) I set the level for excluding a variable quite low. 

The second is to clean out predictors with low variance and hence are not useful in building the model. This is done using the nearZeroVar() function.

Furthermore I have deleted some columns (1 to 7) since these wont be used in the model (including names etc.)
```{r}
## 2.2 Cleaning the data
trainData <- createDataPartition(train$classe, p = 0.6,  list = FALSE)
trainTrain <- train[trainData,]
trainTest <- train[-trainData,]

## Remove predictors with NA's > 15 %
trainTrain <- trainTrain[, colMeans(is.na(trainTrain)) <= .15] 
trainTest <- trainTest[, colMeans(is.na(trainTest)) <= .15] 
testing <- testing[, colMeans(is.na(testing)) <= .15] 

## Remove predictors with near zero variance
nsv <- nearZeroVar(trainTrain, saveMetrics = TRUE)
trainTrain <- trainTrain[,!nsv[,c(4)]]
trainTest <- trainTest[,!nsv[,c(4)]]
testing <- testing[,!nsv[,c(4)]]

## Remove the first seven columns since they are not to use in the model
trainTrain <- trainTrain[,-c(1:7)]
trainTest <- trainTest[,-c(1:7)]
testing <- testing[,-c(1:7)]

## Check dimensions of the cleaned data
dim(trainTrain)
dim(trainTest)
dim(testing)
```

We notice that the dimensions of the cleaned training set is now 11776 objects and 52 variables (compared to 160 before) after slicing and cleaning.

### 2.3 Training and predicting with the training set
During the course different training models are discussed, I will use two of the models discussed in the course which are the Random Forest ("rf") and the Classification Tree ("rpart") due to the classification problem.

The models are supposed to predict the "classe" variable which is in the manner in which they performed the exercise.

Using preProcess I center and scale all the data to have zero mean and unit variance. Large variations in mean and variance will supposedly make the training algorithm not perform as well as with scaled and centered data.

Cross-validation is used with 5-fold groups, again this is somewhat an arbitrary choice.
```{r} 
## 2.3 Training and predictin with the training set
modRf <- train(classe ~ ., data = trainTrain, preProcess=c("center", "scale"), 
               trControl=trainControl(method = "cv", number = 5), method = "rf")
modRpart <- train(classe ~ ., data = trainTrain, preProcess=c("center", "scale"), 
               trControl=trainControl(method = "cv", number = 5), method = "rpart")

predRf <- predict(modRf, trainTest)
predRpart <- predict(modRpart, trainTest)
```

### 2.4 Evaluating which model to use
Here I present a comparison of the two built models using the function confusionMatrix which evaluate the accuracy on the given test sets.
```{r}
## Evaluating which model to use
confusionMatrix(predRf, trainTest$classe)
confusionMatrix(predRpart, trainTest$classe)
```
There is a big difference in the accuracy between the classification tree (0.6027) and the random forest (0.9929). Given the higher accuracy on the random forest model we will use the random forest model (modRf) to predict the outcomes on the real test set. Due to the high accuracy we should expect 20 correct answers on the real test using random forest prediction.

### 2.5 Predicting on "real" test set
Finally the models are used to predict in the outcomes on the real test set given.
```{r}
## 2.5 Predicing on "real" test set
print(predict(modRf, testing))
print(predict(modRpart, testing))
```
As mentioned before, given the higher accuracy from the random forest I will present the predicted outcome as the result from the random forest (modRf). Uploading the data of the 20 test sets gave 20/20 correct answers, which is in line with the estimated accuracy (0.9929) of the model.

## 3 Discussion
The random forest model did perform really well. However, there are some things I think is worth discussing.

Due to the classification problem I quite fast did chose the random forest and the classification tree as methods to use.

Why did I chose the 15 % level of excluding Na's? Excluding to many predictors will reduce the predictors, eventually we could miss out important predictors in building the model. Having many Na's would require some good statistical way to handle the many Na's which then introduces another variance in the model. As far as I know having a large data set gave us the possibility to have a quite low limit on excluding the Na's. Here i would like to have some discussion.

Why cross-validation argument in train-function for random forest? Using random forest cross-validation should not be needed. Every tree uses its own bootstrap. However i found out that the computational time was significantly reduced using 5 folds. Here I am not sure about what is going on. Maybe the five smaller folds are together faster to compute than one big set and hence reduces the computational time. Again, here i would like to have some discussion going on.

Please, do not be afraid to give feedback on this project. I like constructive feedback. Contact info found at the top of the document.